## 🔍 Local RAG with LLMs: Minimal, Flexible, and Fast

This project offers a lightweight setup for Retrieval-Augmented Generation (RAG) using local LLMs and vector databases — ideal for prototyping, testing, and deploying custom knowledge workflows without cloud dependencies.

Features include:
- Local document ingestion and embedding
- Fast retrieval with FAISS
- Streamlined LLM integration (e.g., Ollama, LM Studio)
- Minimal dependencies and clear workflow diagrams

For setup instructions, usage examples, and technical notes, see the full Gist:  
👉 [https://gist.github.com/abiyug/local-ai-rag](https://gist.github.com/abiyug/local-ai-rag)

